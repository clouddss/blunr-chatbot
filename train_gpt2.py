import json
import torch
from transformers import (
    GPT2LMHeadModel,
    GPT2Tokenizer,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling
)
from datasets import Dataset
import numpy as np
from tqdm import tqdm

print("="*60)
print("ğŸš€ TRÃ„NAR GPT-2 PÃ… DIN CREATOR-DATA")
print("="*60)

# AnvÃ¤nd GPT-2 small fÃ¶r snabbare trÃ¤ning
MODEL_NAME = "gpt2"
OUTPUT_DIR = "./creator-gpt2-model"

print(f"\nğŸ“š Laddar modell: {MODEL_NAME}")
tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)
model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)

# SÃ¤tt padding token
tokenizer.pad_token = tokenizer.eos_token
model.resize_token_embeddings(len(tokenizer))

# LÃ¤gg till speciella tokens fÃ¶r att markera roller
special_tokens = {
    "additional_special_tokens": ["<user>", "<assistant>", "<end>"]
}
tokenizer.add_special_tokens(special_tokens)
model.resize_token_embeddings(len(tokenizer))

print("\nğŸ“‚ Laddar trÃ¤ningsdata...")
conversations = []
with open('training_data.jsonl', 'r', encoding='utf-8') as f:
    for line in f:
        data = json.loads(line)
        # Formatera som: <user> frÃ¥ga <assistant> svar <end>
        user_msg = data['messages'][0]['content'].strip()
        assistant_msg = data['messages'][1]['content'].strip()
        
        # Skapa trÃ¤ningstext
        text = f"<user> {user_msg} <assistant> {assistant_msg} <end>"
        conversations.append(text)

print(f"âœ… Laddade {len(conversations)} konversationer")

# AnvÃ¤nd ALLA konversationer fÃ¶r trÃ¤ning
print(f"ğŸ“Š TrÃ¤nar pÃ¥ ALLA {len(conversations)} konversationer!")

# Skapa dataset
dataset = Dataset.from_dict({"text": conversations})

# Tokenisera
def tokenize_function(examples):
    return tokenizer(
        examples["text"], 
        truncation=True, 
        padding="max_length",
        max_length=128  # Kortare fÃ¶r snabbare trÃ¤ning
    )

print("\nğŸ”¤ Tokeniserar data...")
tokenized_dataset = dataset.map(tokenize_function, batched=True)

# Dela i trÃ¤ning och validering
split = tokenized_dataset.train_test_split(test_size=0.1)
train_dataset = split['train']
eval_dataset = split['test']

print(f"ğŸ“ˆ TrÃ¤ningsdata: {len(train_dataset)} exempel")
print(f"ğŸ“‰ Valideringsdata: {len(eval_dataset)} exempel")

# Data collator
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,
)

# TrÃ¤ningsargument - optimerade fÃ¶r alla 26k konversationer
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    overwrite_output_dir=True,
    num_train_epochs=2,  # 2 epoker fÃ¶r 26k data
    per_device_train_batch_size=16,  # StÃ¶rre batch fÃ¶r effektivitet
    per_device_eval_batch_size=16,
    gradient_accumulation_steps=1,
    warmup_steps=500,
    logging_steps=200,
    save_steps=1000,
    eval_strategy="steps",
    eval_steps=1000,
    save_total_limit=2,
    learning_rate=3e-5,
    fp16=torch.cuda.is_available(),
    push_to_hub=False,
    report_to=[],
    load_best_model_at_end=True,
)

# Skapa trainer
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
)

print("\n" + "="*60)
print("ğŸ¯ STARTAR TRÃ„NING MED ALLA 26K KONVERSATIONER!")
print("Detta kommer ta cirka 20-30 minuter...")
print("Modellen kommer lÃ¤ra sig creators exakta stil!")
print("="*60 + "\n")

# TrÃ¤na!
trainer.train()

print("\nğŸ’¾ Sparar trÃ¤nad modell...")
trainer.save_model(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

# Spara konfiguration
config = {
    "model_name": MODEL_NAME,
    "training_samples": len(train_dataset),
    "epochs": training_args.num_train_epochs,
    "special_tokens": special_tokens
}

with open(f"{OUTPUT_DIR}/training_config.json", "w") as f:
    json.dump(config, f, indent=2)

print("\n" + "="*60)
print("âœ… TRÃ„NING KLAR!")
print(f"ğŸ“ Modell sparad i: {OUTPUT_DIR}")
print("="*60)

# Testa modellen
print("\nğŸ§ª Testar trÃ¤nad modell...")

def generate_response(prompt, max_length=50):
    input_text = f"<user> {prompt} <assistant>"
    inputs = tokenizer.encode(input_text, return_tensors='pt')
    
    with torch.no_grad():
        outputs = model.generate(
            inputs,
            max_length=inputs.shape[1] + max_length,
            temperature=0.8,
            do_sample=True,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.encode("<end>")[0]
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=False)
    # Extrahera bara assistant-svaret
    if "<assistant>" in response:
        response = response.split("<assistant>")[1]
        if "<end>" in response:
            response = response.split("<end>")[0]
    
    return response.strip()

# Testa med nÃ¥gra exempel
test_prompts = [
    "Hey beautiful",
    "What are you doing?",
    "Tell me something hot",
    "I miss you"
]

print("\nğŸ“ Exempel pÃ¥ genererade svar:\n")
for prompt in test_prompts:
    response = generate_response(prompt)
    print(f"User: {prompt}")
    print(f"AI: {response}\n")